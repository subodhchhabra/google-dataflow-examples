{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Hello-world\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Hello world</a></div><div class=\"lev1\"><a href=\"#Hello-world-(with-Map)\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Hello world (with Map)</a></div><div class=\"lev1\"><a href=\"#Hello-world-(with-FlatMap)\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hello world (with FlatMap)</a></div><div class=\"lev1\"><a href=\"#Hello-world-(with-FlatMap-and-yield)\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Hello world (with FlatMap and yield)</a></div><div class=\"lev1\"><a href=\"#Counting-words\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Counting words</a></div><div class=\"lev1\"><a href=\"#Counting-words-with-GroupByKey\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Counting words with GroupByKey</a></div><div class=\"lev1\"><a href=\"#Type-hints\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Type hints</a></div><div class=\"lev1\"><a href=\"#BigQuery\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>BigQuery</a></div><div class=\"lev1\"><a href=\"#Combiner-Examples\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Combiner Examples</a></div><div class=\"lev1\"><a href=\"#More-Examples\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>More Examples</a></div><div class=\"lev1\"><a href=\"#Organizing-Your-Code\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Organizing Your Code</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world\n",
    "\n",
    "Create a transform from an iterable and use the pipe operator to chain transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7fde7bf6da90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard imports\n",
    "import google.cloud.dataflow as df\n",
    "# Create a pipeline executing on a direct runner (local, non-cloud).\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "# Create a PCollection with names and write it to a file.\n",
    "(p\n",
    " | df.Create('add names', ['Ann', 'Joe'])\n",
    " | df.Write('save', df.io.TextFileSink('./output/names')))\n",
    "# Execute the pipeline.\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ann\r\n",
      "Joe\r\n"
     ]
    }
   ],
   "source": [
    "!head ./output/names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world (with Map)\n",
    "\n",
    "The <mark>Map</mark> transform takes a callable, which will be applied to each element of the input <mark>PCollection</mark> and must return an element to go into the output <mark>PCollection</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7fde8deedb10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.dataflow as df\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "# Read file with names, add a greeting for each, and write results.\n",
    "(p\n",
    " | df.Read('load messages', df.io.TextFileSource('./output/names'))\n",
    " | df.Map('add greeting',\n",
    "          lambda name, msg: '%s %s!' % (msg, name),\n",
    "          'Hello')\n",
    " | df.Write('save', df.io.TextFileSink('./output/greetings')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Ann!\r\n",
      "Hello Joe!\r\n"
     ]
    }
   ],
   "source": [
    "!head ./output/greetings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world (with FlatMap)\n",
    "\n",
    "A <mark>FlatMap</mark> is like a <mark>Map</mark> except its callable returns a (possibly empty) iterable of elements for the output <mark>PCollection</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7fde8deede90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.dataflow as df\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "# Read previous file, add a name to each greeting and write results.\n",
    "(p\n",
    " | df.Read('load messages', df.io.TextFileSource('./output/names'))\n",
    " | df.FlatMap('add greetings',\n",
    "              lambda name, msgs: ['%s %s!' % (m, name) for m in msgs],\n",
    "              ['Hello', 'Hola'])\n",
    " | df.Write('save', df.io.TextFileSink('./output/greetings')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Ann!\r\n",
      "Hola Ann!\r\n",
      "Hello Joe!\r\n",
      "Hola Joe!\r\n"
     ]
    }
   ],
   "source": [
    "!head ./output/greetings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world (with FlatMap and yield)\n",
    "\n",
    "The callable of a <mark>FlatMap</mark> can be a generator, that is, a function using <mark>yield</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7fde7b60bb90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.dataflow as df\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "# Add greetings using a FlatMap function using yield.\n",
    "def add_greetings(name, messages):\n",
    "  for m in messages:\n",
    "    yield '%s %s!' % (m, name)\n",
    "\n",
    "(p\n",
    " | df.Read('load names', df.io.TextFileSource('./output/names'))\n",
    " | df.FlatMap('greet', add_greetings, ['Hello', 'Hola'])\n",
    " | df.Write('save', df.io.TextFileSink('./output/greetings')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Ann!\r\n",
      "Hola Ann!\r\n",
      "Hello Joe!\r\n",
      "Hola Joe!\r\n"
     ]
    }
   ],
   "source": [
    "!head ./output/greetings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words\n",
    "\n",
    "This example counts the words in a text and also shows how to read a text file from Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7fde7b630250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import google.cloud.dataflow as df\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "(p\n",
    " | df.Read('read',\n",
    "           df.io.TextFileSource(\n",
    "           'gs://dataflow-samples/shakespeare/kinglear.txt'))\n",
    " | df.FlatMap('split', lambda x: re.findall(r'\\w+', x))\n",
    " | df.combiners.Count.PerElement('count words')\n",
    " | df.Write('write', df.io.TextFileSink('./output/results')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'wants', 1)\r\n",
      "(u'whose', 15)\r\n",
      "(u'Duke', 8)\r\n",
      "(u'helps', 1)\r\n",
      "(u'disclaim', 1)\r\n",
      "(u'Mum', 1)\r\n",
      "(u'shell', 1)\r\n",
      "(u'gone', 17)\r\n",
      "(u'battles', 1)\r\n",
      "(u'between', 9)\r\n"
     ]
    }
   ],
   "source": [
    "!head ./output/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words with GroupByKey\n",
    "\n",
    "Here we use <mark>GroupByKey</mark> to count the words. This is a somewhat forced example of <mark>GroupByKey</mark>; normally one would use the transform <mark>df.combiners.Count.PerElement</mark>, as in the previous example. The example also shows the use of a wild-card in specifying the text file source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7fde7a6900d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import google.cloud.dataflow as df\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "class MyCountTransform(df.PTransform):\n",
    "  def apply(self, pcoll):\n",
    "    return (pcoll\n",
    "    | df.Map('one word', lambda w: (w, 1))\n",
    "    # GroupByKey accepts a PCollection of (w, 1) and\n",
    "    # outputs a PCollection of (w, (1, 1, ...))\n",
    "    | df.GroupByKey('group words')\n",
    "    | df.Map('count words', lambda (word, counts): (word, len(counts))))\n",
    "\n",
    "(p\n",
    " | df.Read('read', df.io.TextFileSource('./output/names*'))\n",
    " | df.FlatMap('split', lambda x: re.findall(r'\\w+', x))\n",
    " | MyCountTransform()\n",
    " | df.Write('write', df.io.TextFileSink('./output/results')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Ann', 1)\r\n",
      "(u'Joe', 1)\r\n"
     ]
    }
   ],
   "source": [
    "!head ./output/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type hints\n",
    "\n",
    "In some cases, you can improve the efficiency of the data encoding by providing type hints. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7f43ce3f2210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.dataflow as df\n",
    "from google.cloud.dataflow.typehints import typehints\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "(p\n",
    " | df.Read('A', df.io.TextFileSource('./output/names'))\n",
    " | df.Map('B1', lambda x: (x, 1)).with_output_types(typehints.KV[str, int])\n",
    " | df.GroupByKey('GBK')\n",
    " | df.Write('C', df.io.TextFileSink('./output/results')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Ann', [1])\r\n",
      "(u'Joe', [1])\r\n"
     ]
    }
   ],
   "source": [
    "!head ./output/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery\n",
    "\n",
    "Here is a pipeline that reads input from a BigQuery table and writes the result to a different table. This example calculates the number of tornadoes per month from weather data. To run it you will need to provide an output table that you can write to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Query running...\n",
      "Query done.\n",
      "Processed: 8.8 kb\n",
      "\n",
      "Retrieving results...\n",
      "Got 10 rows.\n",
      "\n",
      "Total time taken 0.89 s.\n",
      "Finished at 2016-05-14 11:16:04.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>tornado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month tornado\n",
       "0      5   False\n",
       "1     10   False\n",
       "2      3   False\n",
       "3     11    True\n",
       "4      2   False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = \"SELECT month, tornado FROM [clouddataflow-readonly:samples.weather_stations] LIMIT 10\"\n",
    "\n",
    "df = pd.read_gbq(query, project_id='YOUR-PROJECT', private_key='YOUR-PRIVATE-KEY')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7f43ede86cd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.dataflow as df\n",
    "input_table = 'clouddataflow-readonly:samples.weather_stations'\n",
    "project = 'YOUR-PROJECT'\n",
    "output_table = 'DATASET.TABLENAME'\n",
    "p = df.Pipeline(argv=['--project', project])\n",
    "(p\n",
    " | df.Read('read', df.io.BigQuerySource(input_table))\n",
    " | df.FlatMap(\n",
    "     'months with tornadoes',\n",
    "     lambda row: [(int(row['month']), 1)] if row['tornado'] else [])\n",
    " | df.CombinePerKey('monthly count', sum)\n",
    " | df.Map('format', lambda (k, v): {'month': k, 'tornado_count': v})\n",
    " | df.Write('write', df.io.BigQuerySink(\n",
    "      output_table,\n",
    "      schema='month:INTEGER, tornado_count:INTEGER',\n",
    "      create_disposition=df.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "      write_disposition=df.io.BigQueryDisposition.WRITE_TRUNCATE)))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\r\n",
      "| month | tornado_count |\r\n",
      "+-------+---------------+\r\n",
      "|     2 |             7 |\r\n",
      "|     4 |             5 |\r\n",
      "|     3 |             6 |\r\n",
      "|    10 |            10 |\r\n",
      "|    12 |            10 |\r\n",
      "|     8 |             4 |\r\n",
      "|     7 |             8 |\r\n",
      "|     1 |            16 |\r\n",
      "|     5 |             6 |\r\n",
      "|     6 |             5 |\r\n",
      "+-------+---------------+\r\n"
     ]
    }
   ],
   "source": [
    "!bq head -n 10 YOUR-PROJECT:DATASET.TABLENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a pipeline that achieves the same functionality, i.e., calculates the number of tornadoes per month, but uses a query to filter out input instead of using the whole table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7f43c5126210>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.dataflow as df\n",
    "project = 'YOUR-PROJECT'\n",
    "output_table = 'DATASET.TABLENAME'\n",
    "input_query = 'SELECT month, COUNT(month) AS tornado_count ' \\\n",
    "        'FROM [clouddataflow-readonly:samples.weather_stations] ' \\\n",
    "        'WHERE tornado=true GROUP BY month'\n",
    "p = df.Pipeline(argv=['--project', project])\n",
    "(p\n",
    "| df.Read('read', df.io.BigQuerySource(query=input_query))\n",
    "| df.Write('write', df.io.BigQuerySink(\n",
    "    output_table,\n",
    "    schema='month:INTEGER, tornado_count:INTEGER',\n",
    "    create_disposition=df.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "    write_disposition=df.io.BigQueryDisposition.WRITE_TRUNCATE)))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\r\n",
      "| month | tornado_count |\r\n",
      "+-------+---------------+\r\n",
      "|     5 |             6 |\r\n",
      "|     1 |            16 |\r\n",
      "|     3 |             6 |\r\n",
      "|     6 |             5 |\r\n",
      "|     2 |             7 |\r\n",
      "|     9 |             7 |\r\n",
      "|    12 |            10 |\r\n",
      "|     4 |             5 |\r\n",
      "|    11 |             9 |\r\n",
      "|     7 |             8 |\r\n",
      "+-------+---------------+\r\n"
     ]
    }
   ],
   "source": [
    "!bq head -n 10 YOUR-PROJECT:DATASET.TABLENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combiner Examples\n",
    "\n",
    "A common case for Dataflow combiners is to sum (or max or min) over the values of each key. Such standard Python functions can be used directly as combiner functions. In fact, any function \"reducing\" an iterable to a single value can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x7f43ce317150>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.dataflow as df\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "SAMPLE_DATA = [('a', 1), ('b', 10), ('a', 2), ('a', 3), ('b', 20)]\n",
    "\n",
    "(p\n",
    " | df.Create(SAMPLE_DATA)\n",
    " | df.CombinePerKey(sum)\n",
    " | df.Write(df.io.TextFileSink('./output/results')))\n",
    "p.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 6)\r\n",
      "('b', 30)\r\n"
     ]
    }
   ],
   "source": [
    "!head ./output/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `google/cloud/dataflow/examples/cookbook/combiners_test.py` file in the source distribution contains more combiner examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Examples\n",
    "\n",
    "The `google/cloud/dataflow/examples` subdirectory in the source distribution has some larger examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing Your Code\n",
    "\n",
    "Many projects will grow to multiple source code files. It is beneficial to organize the project so that all the code involved in running a workflow can be built as a Python package so that it can be installed in the VM workers executing a job.\n",
    "\n",
    "Please follow the example in `google/cloud/dataflow/examples/complete/juliaset`. If the code is organized in this fashion then you can use the --setup_file command line option to create a source distribution out of the project files, stage the resulting tarball and later install it in the workers executing the job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_section_display": "none",
   "toc_threshold": 6,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
